from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

app = FastAPI()

# ğŸ”¹ CORS ì„¤ì • ì¶”ê°€ (ëª¨ë“  ì¶œì²˜ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©í•˜ë ¤ë©´ ["http://yourdomain.com"]
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì„œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

# Langchain Ollama ì‚¬ìš©
llm = ChatOllama(
    model="hf.co/mradermacher/Qwen2.5-14B-Gutenberg-1e-Delta-GGUF:Q4_K_S",
    format="json",
    temperature=0,
)


@app.get("/")
async def root():
    return {"message": "Hello World"}


@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    user_input = data.get("message", "")

    if not user_input:
        return {"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!"}

    prompt = ChatPromptTemplate.from_messages(["{user_input}"])
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"user_input": user_input})

    return {"response": response}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
